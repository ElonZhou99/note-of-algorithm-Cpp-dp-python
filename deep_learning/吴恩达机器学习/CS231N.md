[TOC]

# 一、Image Classification 图像分类

## 1.1 K-Nearest Neighbors K最邻近法

（常不适用于图像分类）

距离度量distance metric:

L1distance / L2distance

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gm3lb8wfvvj314q0jqaig.jpg" alt="截屏2020-12-28 下午3.13.58" style="zoom:50%;" />超参数hyperparameters:

1. k
2. distance

Terrible idea:

1. 全部数据用于训练，调参至完美拟合数据（k=1）
2. 数据集分为训练集和测试数据集

terrible reason:

1. 应分为训练集train data，验证集validation data，测试集test data
2. 应增大k，使预测未知数据更加准确，提高泛化能力

==Cross-Validation交叉验证：==

Split data into fold,

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkzg73wdxtj31180hmtb3.jpg" alt="截屏2020-11-20 上午12.09.07" style="zoom:50%;" />

Problem:

Curse of dimensionality维度灾难

## 1.2 Linear Classification. 线性分类

Ex:输入图像——输出描述性语言

1. Convolutional（卷积） neural network ——图像
2. Recurrent（循环） neural network——语言

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gm2q1gdi8dj31200iok3m.jpg" alt="截屏2020-11-20 上午10.51.48" style="zoom:50%;" />

$f(x,W)$: 10种分类的概率10*1

$W$: 10种分类的全部特征变量的权重10*3072

$x$: 1个图像有3027个特征变量3027*1

$b$: 10种分类的偏差 10*1

Ex：猫、狗、船三种类型的图像，每张图像四个像素点

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gm2q5z0rw1j312a0h6tk2.jpg" alt="截屏2020-11-20 上午10.57.44" style="zoom:50%;" />

将计算得到的权重W代入模型，使分类器可视化

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gm2qx0r7u1j312s0j0av7.jpg" alt="截屏2020-11-20 上午10.58.25" style="zoom:50%;" />

==note==:使用线性分类，每个类别只能学习一种模板，泛化能力差。解决不了复杂的决策边界、多维和多分类问题。

# 二、Loss Function and Optimization损失函数和优化

损失函数：定量的估计模型权重W的好坏，代入模型求预测值和实际标签的偏差

输入数据集：{${(x_i,y_i)}$}$^N_{i=1}$

​		$x_i$：图像i	$y_i$：标签i

损失函数$L_i$

$L={1\over N}\sum_i^nL_i(f(x_i,y_i),y_i)$

对每个样本都求损失函数再求平均

## 2.1 SVM loss

支持向量机：只计算分类错误分类对应的损失，再求和

$L_i=\sum_{j\not =y_i}max(0,S_j-S_{y_i}+1)$

$j \not=y_i$：计算每个错误分类的分数对应的损失，再求和

$S_j$：分类器预测某一分类的得分

$S_{y_i}$：训练集中第i个样本的得分值（真实标签）

$+1$：安全距离（可变）

$S_j-S_{y_i}+1$：预测值 - 真实值或预测正确分数 + 安全距离

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gm2r3cd8yaj31fk0mung1.jpg" alt="截屏2020-11-20 下午3.39.35" style="zoom:50%;" />

Loss of cat : $max(0, 5.1-3.2+1)+max(0,-1.7-3.2+1)=2.9+0=2.9$

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gm2r3ul0uhj31bm0nqaq4.jpg" alt="截屏2020-11-21 上午10.51.06" style="zoom:50%;" />

==note==: SVM Loss只关注正确分类得分比错误分类得分要大于一个安全距离，并不关注具体差距是多少

==questions==:

损失函数是量化不同的错误分类误差有多大，为啥不用平方损失函数

平方损失函数：放大误差

SVM loss：忽略小错误，放大大错误？？？

==最优解的W不唯一==

==损失函数正则化==：让模型选择简单的W参数，防止过拟合

$L(w)={1\over N}\sum_{i=1}^NL_i(f(x_i,W),y_i)+\lambda R(w)$

$L(w)$ = Data loss + Regularzation(使模型更简单)

1、 限制模型，防止出现高阶导致太复杂

2、加入软惩罚项，模型仍能很好的拟合数据，且效果逼近高阶复杂模型

In come use:

L2 regularization: $R(w)=\sum_k\sum_lW_{k,l}^2$

L1 regularization: $R(w)=\sum_k\sum_l|W_{k,l}|$

Elastic net(L1+L2): $R(w)=\sum_k\sum_l\beta W_{k,l}^2+|W_{k,l}|$

L2：对权重向量的欧式范数进行惩罚

L1：鼓励稀疏

==question==

L2: 可用一种相对粗糙的方式度量复杂模型，更加能传递x中不同元素的影响，鲁棒性更好

L1:更倾向于首元素为1，其他为0的w

## 2.2 Softmax Classifier

$L_i=-logP(Y=y_j|X=x_j)=-log({e^{S_{y_i}}\over \sum _je^{S_j}})$

概率分布越大，损失函数Li越小

$S_{y_j}$：正确预测得分（真实值）

$S_j$ ：预测得分

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gm2v5s71shj311s0hwn7r.jpg" alt="截屏2020-11-21 下午9.33.13" style="zoom:50%;" />

1. 代入w求预测值y，得到各个分类得分
2. 指数化
3. 归一化
4. 求对数，取负数

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gm2v7z18suj31280iowy5.jpg" alt="截屏2020-11-21 下午9.40.22" style="zoom:50%;" />

==note==

softmax损失函数最大值为无穷，最小值为0

## 2.3 SVM loss VS. Softmax loss

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gm3l8ey3suj315y0k64j2.jpg" alt="截屏2020-12-28 下午3.11.14" style="zoom:50%;" />

SVM loss：只关注正确分类概率要大于错误分类的概率，不关注错误分类的小变化

Softmax loss：目标是将概率质量函数（离散分布值）等于1。改变正确分类和错误分类（往好的方向），Softmax将持续变化，将正确分类的得分推向无穷大，错误分类得分推向无穷小（追求更好）

Ex: 

dataset数据集：$(x, y)$

Score function 假设函数：$s=f(x;w)=Wx$

Loss function损失函数：

​	Softmax ：$L_i=-log({e^{S_{y_j}}\over \sum_je^{S_j}})$

​	SVM ：$L_i=\sum_{j\not=y_j}max(0,S_j-S_{y_j}+1)$

​	full loss：$L={1\over N}\sum_{i=1}^NL_i+R(w)$

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gm9gvyrujbj312g0h8qe5.jpg" alt="截屏2020-11-21 下午9.56.11" style="zoom:50%;" />

## 2.4 Optimization 优化

梯度下降

Gradient梯度：偏导数组成的向量

​	和特征变量矩阵X的形状一致，代表某一特征变量方向上的函数f的斜率

确定一点任意方向的斜率：

​	这一点上梯度和该点单位方向向量的点积

求梯度的方法：

1. 有限差分法：（慢）

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmax3fionbj31420lqwws.jpg" alt="截屏2021-01-03 下午11.20.31" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmax3wp2idj313q0m01b4.jpg" alt="截屏2021-01-03 下午11.21.04" style="zoom:50%;" />

2. 微积分法：（快，只需用表达式）

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmax58ewcsj31400lm4eg.jpg" alt="截屏2021-01-03 下午11.22.18" style="zoom:50%;" />

总结：

Numberical gradient 数值梯度（有限差分法）：简单、有意义、慢、用来debug

Analytic gradient 解析梯度（微积分法）：准确、快速、易错

==note==

调参时先找到合适的学习率或步长

==Stachastic Gradient Descent (SGD) 随机梯度下降==

N（样本数）很大，迭代一次计算成本过高，使用minibatch小批量梯度下降32/64/128/256

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxa2i6k3j313m0la7m6.jpg" alt="截屏2021-01-03 下午11.26.59" style="zoom:50%;" />

==note== Image Features

运用大规模深度学习网络之前：

1. 转化特征，使复杂模型简化后传入网络

   例如颜色直方图

   <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxd91ieaj313w0mewv5.jpg" alt="截屏2021-01-03 下午11.30.01" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxdr1bpzj31420lm4ba.jpg" alt="截屏2021-01-03 下午11.30.31" style="zoom:50%;" />



<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxeu18bzj31400lyh8e.jpg" alt="截屏2021-01-03 下午11.31.32" style="zoom:50%;" />





# 三、Neural Networks

## 3.1 Backpropagation

任意复杂函数的解析梯度

Computational graphs计算图

==Hinge loss合叶损失（SVM）==

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkzz7x3rqej313a0eqtfi.jpg" alt="截屏2020-11-24 上午8.52.32" style="zoom:50%;" />



反向传播是链式法则chain rule的递归调用/含中间变量的求偏导数

e.g. a simple example

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl009izpicj315m0kmdt9.jpg" alt="截屏2020-11-24 上午9.28.43" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl00e37x41j316i0judv4.jpg" alt="截屏2020-11-24 上午9.33.03" style="zoom:50%;" />

another example:

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl00sk9avmj31680k27f0.jpg" alt="截屏2020-11-24 上午9.46.59" style="zoom:50%;" />

Upstream gradient times this gradient





==Sigmoid function==:  $=\sigma(z)={1\over 1+e^{-z}}=$

${d\sigma(z)\over dz}={e^{-z}\over (1+e^{-z})^2}=({1+e^{-z}-1\over 1+e^{-z}})({1\over 1+e^{-z}})=(1-\sigma(z))\sigma(z)$

<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gl01brqxcpj319s0lqk57.jpg" alt="截屏2020-11-24 上午10.05.06" style="zoom:50%;" />

Q: max gate : gradient router	max门的梯度

A: the max one is 1 , the min one is 0

Q: multiplication gate : gradient switcher 

A: get upstream gradient and scale it by the value of the other branch上游梯度对另一个分支的缩放

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1glg7k1jvrwj31bc0p41kx.jpg" alt="截屏2020-12-08 上午9.49.57" style="zoom:50%;" />

==Vectorized向量化（雅可比矩阵）==

矩阵每个元素是输出向量的每个元素对输入向量每个元素分别求偏导数的结果

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl02r8jkt7j31ca0n87mq.jpg" alt="截屏2020-11-24 上午10.54.49" style="zoom:50%;" />

Vectorized:  输入一个有4096个特征的变量（样本）

if there are 4096-d input vector 

The f(z) = max(0,x)

4096-d output vector

4096个特征，构成4096种输出

Q: what is the size of the Jacobian matrix?

A: 4096 by 4096

如一次同时输入100个变量（样本）

the Jacobian matrix size is 409600 by 409600

Note: 不需要将雅可比矩阵全部写出进行计算，这里的雅可比可以看作是对角矩阵，只和输出的第一个元素有关



==vectorized example==

$$f(x,W)=q^2=||W·x||^2=\sum^n_{i=1}(W·x)^2_i$$二范数

$x$：n维向量

$W$：n*n矩阵

下图：求两个样本的二范数，一个样本有两个特征，W为两个样本特征变量的权重，x为已知的特征变量。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1glg8gvk2qqj31ci0ng1e9.jpg" alt="截屏2020-12-08 上午10.21.36" style="zoom:50%;" />

反向传播：

1. $f$对$L2$求偏导为1

2. $f$对$q_i$求偏导为$2q_i*1$

3. $f$对$W$求偏导为$2q·X^T\quad$

   ${\partial f \over \partial W_{i,j}}=\sum_k{\partial f\over \partial q_k}{\partial q_k\over \partial W_{i,j}}=\sum_k(2q_k)(1_{k=i}x_j)=2q_iX_j$

4. $f$对$X$求偏导为$2W^T.q$

   ${\partial f \over \partial X_{i}}=\sum_k{\partial f\over \partial q_k}{\partial q_k\over \partial X_{i}}=\sum_k(2q_k)(1_{k=i}W^T)=2q_iW^T$

==矩阵的倒数就是逆矩阵==

前向传播计算节点输出，反向传播中计算梯度

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxw9u1xsj31420igajb.jpg" alt="截屏2021-01-03 下午11.48.17" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxxj0ttoj313w0hidoo.jpg" alt="截屏2021-01-03 下午11.49.32" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gmaxz72820j313g0iyds7.jpg" alt="截屏2021-01-03 下午11.50.38" style="zoom:50%;" />

## 3.2 Neural networks 神经网络

单层神经网络：$f=Wx$

两层神经网络：$f=W_2max(0,W_1x)$

<img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gn0wy1jyxwj30iw06s3zk.jpg" alt="截屏2021-01-26 上午11.00.04" style="zoom:50%;" />

三层神经网络：$f=W_3max(0,W_2max(0,W_1x))$

激活函数应用在神经元端步，得到的值作为输出到相关联的神经元

例如sigmoid激活函数

与神经元反应最为相似的激活函数是relu激活函数

# 四、Convolutional Neural Networks 卷积神经网络

















