[TOC]

# 一、引言inreoduction

机器学习主要分为：监督学习，无监督学习，半监督学习（强化学习）

## 1. 1监督学习Supervised learning

==定义：==

教计算机完成某事，训练数据集Train data有特征Features和标签Labels即包括神经网络Neural network中的输入层Input和输出层Output。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk8c6tpf6bj30ku0bqdhu.jpg" alt="截屏2020-10-31 上午11.05.26" style="zoom:50%;" />

==流程：==

通过给定的训练样本（已知输入层和输出层），训练得出最优的假设函数Hypothesis function$h_\theta(x)$的模型参数parameter（权重weight）$\Theta$，从而得出了$h_\theta(x)$的可计算函数。最后输入测试集Test data的输入层（即特征变量），由$h_\theta(x)$计算得出输出预测

==实例：==

分类问题Classification，回归问题Regression。

房价预测Housing prince prediction，乳腺癌分类



## 1.2 无监督学习Unsupervised learning

==定义：==

计算机自己找到某种结构规律。给定的数据集中无标签，数据类别未知，计算机自己将一个样本集聚类Clustering分为若干个子集，使相同子集中的元素（样本）特征间距小。

==分类：==

1. 概率密度Probability density估计的直接方法：设法找到各类别在特征空间的分布参数Distributed parameter，再进行分类。
2. 基于样本相似性度量的简洁聚类方法：设法定出不同类别的核心或初始内核，然后依据样本与核心之间的相似性度量将样本聚类为不同的类别。

==实例：==

PCA， deep learning的一些算法，数据挖掘Data mining，模式识别Pattern recognition，图像处理Image processing，DNA微观数据，背景音乐提取等

利用无监督学习的聚类结果可以提取样本数据集中的隐藏Hidden信息，对未来数据进行分类和预测Forcast。

# 二、单变量线性回归Linear Regression with One variable

## 2.1 模型表述Model Representantion

单变量线性回归算法：房价预测为例

$m$：训练集样本数量

$x$：样本的特征/输入特征

$y$：目标变量/输出变量（labels）

$(x,y)$：训练集中的样本

$(x^{(i)},y^{(i)})$：训练集中第$i$个样本

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk8i5we8sfj30h206umy9.jpg" alt="截屏2020-10-31 下午2.32.06" style="zoom:50%;" />

其中Size in feet​为特征$x_1$，price为输出变量$y$

## 2.2 算法Algorithm

hypothesis function : $h_\theta(x)=\theta_0+\theta_1x_1$

 Cost function : $J(\theta)={1\over 2m}\sum^m_{i=1}(h_\theta(x)^{(i)}-y^{(i)})^2$

Gradient descent of Linear regression : 

$\theta_0=\theta_0-\alpha{\partial J(\theta)\over \partial\theta_0}=\theta_0-{\alpha\over m}\sum_{i=1}^m(h_\theta(x)^{(i)}-y^{(i)})$

$\theta_1=\theta_1-\alpha{\partial J(\theta)\over \partial\theta_1}=\theta_1-{\alpha\over m}\sum_{i=1}^m[(h_\theta(x)^{(i)}-y^{(i)})x_1^{(i)}]$

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk8hmg3txsj30n60dg485.jpg" alt="截屏2020-10-31 下午2.13.27" style="zoom:50%;" />

式中$\theta_0$为偏移项

假设函数为线性函数，损失函数为二次函数时:

$\theta_1$代表假设函数的斜率，当$\theta_1$的值偏小，模型会处于正确回归线的下方，同时此时该点处于损失函数的递减区域，$\theta_1$此时需要增加。这时候$J(\theta_1)$的斜率为负数，$\theta_1-J(\theta_1)的斜率$为增加。

## 2.3 注意点Attention point

梯度下降gradient descent引入了学习率learning rate，式中用$\alpha$表示。

学习率大小决定损失函数下降程度最大方向的步幅

1. 学习率$\alpha$过大时，可能无法接近最优点，甚至可能发散。
2. 学习率$\alpha$过小时，需要迭代的次数就越多。
3. 在对$\theta_0, \theta_1$迭代时，应同时迭代再更新$\theta$的值
4. 注意对一个批量样本迭代完成后应该清空梯度，也就是将：$\theta_1=\theta_1-{\partial J(\theta)\over \partial\theta_1}=\theta_1-{1\over m}\sum_{i=1}^m[(h_\theta(x)^{(i)}-y^{(i)})x_1^{(i)}]$中的叠加项清空，迭代下一批样本时应从0开始计算。

# 三、多变量线性回归Linear Regression with Multiple variable

## 3.1 模型描述Model Representantion

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk8in5avcfj30he07g408.jpg" alt="截屏2020-10-31 下午2.48.46" style="zoom:50%;" />

其中size为特征变量$x_1$；

number of bedrooms为特征变量$x_2$；

number of floors为特征变量$x_3$；

age为特征变量$x_4$；

price为输出变量$y$

$x^i$上标代表第$i$个样本

## 3.2 算法Algorithm

Hypothesis function : $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\theta_4x_4$

Cost function : $J(\theta)={1\over 2m}\sum^m_{i=1}(h_\theta(x)^{(i)}-y^{(i)})^2$

Gradient descent of Linear Regression : 

$\theta_0=\theta_0-\alpha{\partial J(\theta)\over \partial\theta_0}=\theta_0-{\alpha\over m}\sum_{i=1}^m(h_\theta(x)^{(i)}-y^{(i)})$

$\theta_1=\theta_1-\alpha{\partial J(\theta)\over \partial\theta_1}=\theta_1-{1\over m}\sum_{i=1}^m[(h_\theta(x)^{(i)}-y^{(i)})x_1^{(i)}]$

.........

$\theta_4=\theta_4-\alpha{\partial J(\theta)\over \partial\theta_4}=\theta_4-{\alpha\over m}\sum_{i=1}^m[(h_\theta(x)^{(i)}-y^{(i)})x_4^{(i)}]$

## 3.3 特征缩放Feature Scaling

在训练样本集中，特征$x_j$的取值范围如果相差很大即不同的特征变量尺度不统一，这样会导致梯度下降算法会收敛的很慢。

因此特征缩放（归一化）是十分必要的

将所有的特征缩放到[-1,1]（合适的）之间。
其中默认$x_0=1$已经在范围内。将其他的特征除以不同的数达到合适的（｜3｜｜1/3｜）范围
                   $$x_1\leftarrow{x_1-\mu_1 \over s_1}$$
$\mu$：所有样本中特征x1的平均数
$s_1$：特征x1中max - min

## 3.4 向量化 Vectorization

房价预测为例，特征变量：size , number , floors

Input : {$x_1:size;\quad x_2:number;\quad x_3:floors$}

$\Theta={\begin {bmatrix}\theta_0 \\ \theta_1\\ \theta_2 \\ \theta_3  \end{bmatrix}} \quad    X={\begin{bmatrix}1&x_1^1&x_2^1&x_3^1 \\1&x_1^2&x_2^2&x_3^2\\1&x_1^3&x_2^3&x_3^3\\...\\1&x_1^m&x_2^m&x_3^m  \end {bmatrix}}\quad   $

$  Labels={\begin{bmatrix} label_1 \\ label_2 \\ label_3\\ ...\\lebel_m\end{bmatrix}} \quad Y={\begin{bmatrix}\theta_0+\theta_1x_1^1+\theta_2x_2^1+\theta_3x_3^1\\theta_0+\theta_1x_1^2+\theta_2x_2^2+\theta_3x_3^2\\theta_0+\theta_1x_1^3+\theta_2x_2^3+\theta_3x_3^3\\...\\theta_0+\theta_1x_1^m+\theta_2x_2^m+\theta_3x_3^m \end{bmatrix}}={\begin{bmatrix}y_1\\y_2\\y_3\\...\\y_m \end {bmatrix}}$

Hypothesis function :  $h_\theta(x)=Y=X\Theta$

Cost function : $J(\theta)={1\over 2m}\sum^m_{i=1}(h_\theta(x)^{(i)}-Labels^{(i)})^2={1\over 2m}(X\Theta-Labels)^T(X\Theta-Labels)$

Gradient descent : $\theta_j := \theta_j-\alpha{1\over m}\sum^m_{i=1}[(h_\theta(x)^{(i)}-Labels^{(i)})x^{(i)}_j]=\theta_j-\alpha{1\over m}X_{j列}^T(X\Theta-Labels)$

## 3.5 正则化Regularization

在使用复杂的假设函数时，可能会出现过拟合的现象，泛化能力差。

可通过减少特征变量，减小特征多项式的次数，减小参数$\theta_j$的大小（将它们的值变小可使曲线更加的平滑）

因此加入正则化项即惩罚项。使部分特征变量对函数曲线的影响约等于0。或将所有参数都进行“惩罚”，使曲线平滑。

损失函数：==注意$\theta_0$不需要正则化==
$$J(\Theta)={1\over 2m}[\sum^m_{i=1}(h_\theta(x^{(i)})-Labels^{(i)})^2+\lambda \sum^n_{j=1}\theta^2_j]$$

​          $={1\over 2m}(X\Theta-Labels)^T(X\Theta-Labels)+{\lambda\over 2m}(\Theta^T\Theta-\theta_0^2)$

优化函数（加入正则化项的梯度下降）：
$$\theta_j:=\theta_j(1-\alpha{\lambda \over m})-\alpha {1\over m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$

$\Theta := \Theta(1-\alpha{\lambda\over m})-\alpha{1\over m}X^T(X\Theta-Labels)$

If j == 0:

​	$\lambda=0$

## 3.6 特征和多项式回归Featres and Polynomial Regression

在特征的选择中，避免选择无意义的特征变量。例如选择了房子的长和宽，再选择面积就重复选择了。

线性回归终究是一条直线，在很多的情况下都不能很好的拟合数据，因此这里引入多项式回归。即特征变量最高次大于等于2次。

例如：$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2$

先观察数据，再决定使用怎样的模型。

这里注意多项式回归更加要做特征缩放

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk8k1edcluj31560k8wsk.jpg" alt="截屏2020-10-31 下午3.37.04" style="zoom:50%;" />

## 3.7 正规方程法Normal Equation

通过矩阵的方法直接求参数向量$\Theta$

$\Theta=(X^TX)^{-1}X^Ty$

注意矩阵的形状。

求$(X^TX)$的伪逆

# 四、逻辑回归Logistic Regression

## 4.1 模型描述Model Representention

逻辑回归适合用于分类问题，例如是否患癌症，是为1，否为0，样本为0/1分布。

这里的假设函数$h_\theta(x)$指的是给定参数$\theta$的前提下，$y=1$即患病的概率

条件概率：$h_\theta(x)=P(y=1|x;\Theta)$

逻辑回归：$g(z)={1\over 1+e^{-z}}$

其中$z=\Theta^TX$

$h_\theta(x)=g(z)={1\over 1+e^{-\Theta^TX}}$

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk8yt05afxj30vq0fqtf6.jpg" alt="截屏2020-11-01 上午12.07.59" style="zoom:50%;" />

将模型假设函数设为如上图所示的逻辑回归，例如设置$h_\theta(x)$值大于0.5为患病，小于0.5为不患病。



## 4.2 算法Algorithm

$z=\Theta^TX=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3$

$g(z)={1\over 1+e^{-z}}={1\over 1+e^-(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3)}$

Hypothesis function : $h_\theta(x)=g(z)$

Cost function : 

$J(\Theta)=-{1\over m}\sum^m_{i=1}[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$

Gradient descent of Logistic Regression : 

$\theta_j := \theta_j-\alpha{1\over m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j$

## 4.3 向量化Vactorization

Input : $x_1, x_2,x_3$

$\Theta={\begin{bmatrix}\theta_0\\\theta_1\\\theta_2\\\theta_3 \end{bmatrix}}\quad X={\begin{bmatrix}1&x_1^1&x_2^1&x_3^1\\1&x_1^2&x_2^2&x_3^2\\...\\1&x_1^m&x_2^m&x_3^m \end{bmatrix}}$

$Labels={\begin{bmatrix}label_1\\label_2\\...\\label_m \end{bmatrix}}\quad Z=X\Theta={\begin{bmatrix}\theta_0+\theta_1x_1^1+\theta_2x_2^1+\theta_3x_3^1 \\\theta_0+\theta_1x_1^2+\theta_2x_2^2+\theta_3x_3^2\\...\\\theta_0+\theta_1x_1^m+\theta_2x_2^m+\theta_3x_3^m\end{bmatrix}}$

Hypothesis function : $Y=h_\theta(x)=g(z)={1\over 1+e^{-Z}}={1\over 1+e^{-X\Theta}}$

Cost function : $J(\theta)=-{1\over m}\sum^m_{i=1}Labels^{(i)}log(h_\theta(x^{(i)}))-{1\over m}\sum^m_{i=1}(1-Labels^{(i)})log(1-h_\theta(x^{(i)}))$

​                                  $=-{1\over m}Labels^T(logY)-{1\over m}({\begin{bmatrix}1^1\\1^2\\...\\1^m \end{bmatrix}}-Labels)^T[log({\begin{bmatrix}1^1\\1^2\\...\\1^m \end{bmatrix}}-Y)]$

Gradient descent : $\theta_j:=\theta_j-\alpha{1\over m}X_j^T(Y-Labels)$

​                                 $\Theta=\Theta-\alpha{1\over m}X^T(Y-Labels)$

## 4.4 正则化Regularization

损失函数：
$$J(\theta)=-{1\over m}\sum^m_{i=1}[Labels^{(i)}logh_\theta(x^{(i)})+(1-Labels^{(i)})log(1-h_\theta(x^{(i)}))]+\lambda \sum^n_{j=1}\theta^2_j]$$

​        $=-{1\over m}Labels^T(logY)-{1\over m}({ \begin {bmatrix}1^1 \\1^2 \\...\\1^m \\ \end {bmatrix}}-Labels)^T[log({\begin{bmatrix} 1^1 \\1^2 \\...\\1^m \end{bmatrix}}-Y)]+\lambda(\Theta^T\Theta-\theta_0^2)$

优化函数：加入正则化项的梯度下降：==注意不要对$\theta_0$正则化==
$$\theta_j:=\theta_j(1-\alpha{\lambda \over m})-\alpha {1\over m}\sum^m_{i=1}(h_\theta(x^{(i)})-Labels^{(i)})x_j^{(i)}$$

$\Theta := \Theta(1-\alpha{\lambda\over m})-\alpha{1\over m}X^T(X\Theta-Labels)$

If j == 0:

​	$\lambda=0$    

## 4.5 判定边界Decision Boundary

若果$h(x)>=0.5$,判定会患病即"$y=1$"
反之$h(x)<0.5$,判定不会患病即"$y=0$"
又因为$h_\theta(x)=g(z)=g(\Theta^Tx)={1\over 1+e^{-\Theta^Tx}}$。由图像可知:
当$\Theta^Tx>0$时，$h_\theta(x)=g(\Theta^Tx)>0.5$
当$\Theta^Tx<0$时，$h_\theta(x)=g(\Theta^Tx)<0.5$

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk8z93l3h1j30zm0i2tkm.jpg" alt="截屏2020-11-01 上午12.23.27" style="zoom:50%;" />

通过构造高次多项式的判定边界可以获得复杂的分类边界

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gk8zbfp92pj314y0lknfc.jpg" alt="截屏2020-11-01 上午12.25.40" style="zoom:50%;" />

## 4.6 多类别分类的逻辑回归Logistic Regression of One-vs-all Variable

将需要分类出来的某种类别样本标签设为1，其余的全部样本标签为0。依次完成每个类别的分类。



