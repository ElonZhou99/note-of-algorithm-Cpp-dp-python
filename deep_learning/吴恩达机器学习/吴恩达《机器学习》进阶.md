[TOC]

# 一、神经网络Neural Netwoeks Representation

## 1.1 神经网络的假设 Hypothesis of neural networks

逻辑回归虽然适用于分类模型，但是当分类类别多时，使用逻辑回归会产生特别多的特征变量，会产生过拟合现象。

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkcucia12ej312y0lun91.jpg" alt="截屏2020-11-04 上午8.35.57" style="zoom:50%;" />

50✖️50像素的灰度图像会产生50✖️50✖️255个特征类别，按不同特征类别按最高特征项为二次项进行组合会产生三百万个特征变量。

Input layer输入层

Output layer输出层

Hidden layer隐藏层

Parameters模型参数 = weight权重

$a^{(j)}_i$ ：第$j$层神经网络的第$i$个神经元（激活项）

$\Theta_i^{(j)}$ ：控制从第$j$层到第$j+1$层的第$i$个神经元的映射权重向量

## 1.2 前向传播Forward propagation

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkcumfwd9qj311w0m44ek.jpg" alt="截屏2020-11-04 上午8.45.32" style="zoom:50%;" />

输入层的$x_1,x_2,x_3$是样本特征值，为已知；通过第一层往前算，最后将第三层输出层的计算值和样本实际输出值做对比。

$Z=\Theta^Tx$

$h_\theta(x)=g(z)={1\over 1+e^{-Z}}$

$a_1^{(2)}=g(\Theta^{(1)}_{10}x_0+\Theta^{(1)}_{11}x_1+\Theta^{(1)}_{12}x_2+\Theta^{(3)}_{13}x_3)$

$a_2^{(2)}=g(\Theta^{(1)}_{20}x_0+\Theta^{(1)}_{21}x_1+\Theta^{(1)}_{22}x_2+\Theta^{(3)}_{23}x_3)$

$a_3^{(2)}=g(\Theta^{(1)}_{30}x_0+\Theta^{(1)}_{31}x_1+\Theta^{(1)}_{32}x_2+\Theta^{(3)}_{33}x_3)$

$h_\theta(x)=g(\Theta^{(2)}_{10}a_0^{(2)}+\Theta^{(2)}_{11}a_1^{(2)}+\Theta^{(2)}_{12}a^{(2)}_2+\Theta^{(2)}_{13}a_3^{(2)})$

向量化Vectorized :

$x={\begin{bmatrix}x_0 \\ x_1\\x_2\\x_3 \end{bmatrix}}\quad Z^{(2)}={\begin{bmatrix} Z_1^{(2)}\\ Z_2^{(2)}\\Z_3^{(2)}\end{bmatrix}}$

第一层到第二层：

$Z^{(2)}=\Theta^{(1)}a^{(1)}=\Theta^{(1)}x$

$a^{(2)}=g(Z^{(2)})$

第二层到第三层（输出层）：

添加$a_0^{(2)}=1$偏移项

$Z^{(3)}=\Theta^{(2)}a^{(2)}$

$h_\Theta(x)=a^{(3)}=g(Z^{(3)})$

<img src="https://img-blog.csdnimg.cn/20201026011105271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVySmF5enox,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />

## 1.3 与、或、非、运算and\or\not

And与运算：

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkcv60hwlxj311q0jwh1o.jpg" alt="截屏2020-11-04 上午9.04.20" style="zoom:50%;" />

Or或运算

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkcv6xbvmsj31060gqn4l.jpg" alt="截屏2020-11-04 上午9.05.14" style="zoom:50%;" />

Not非运算

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkcv77dchaj310w0hiaiz.jpg" alt="截屏2020-11-04 上午9.05.29" style="zoom:50%;" />

## 1.4 多元分类Multiple output units: One-vs-all

输出层由多个上神经元组成，将其拼凑成一个0/1元素组成的向量。例如：

<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkcve4ujesj311i0l8k79.jpg" alt="截屏2020-11-04 上午9.12.08" style="zoom:50%;" />

# 二、神经网络Neural Network

## 2.1 神经网络损失函数Cost function

神经网络数据集：${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})}$

神经网络层数：$L$

第$l$层的神经元数（$l$层特征数）：$s_l$（不包括偏差特征）

$\sum^m_{i=1}$: 样本从1到m的累加

$\sum_{k=1}^K$: 输出类别从1到K的累加

$\sum^{L-1}_{l=1}$: 神经网络从第1层到第L-1层的累加

$\sum_{i=1}^{s_l}$: 第$l$层神经网络中的神经元从1到$s_l$的累加

$\sum^{s_l+1}_{j=1}$: 第$l$层中第$i$个神经元的特征$\theta_{ij}$从$j=0$到$j=s_l+1$的特征累加

注意不要将$\Theta_{i0}$正则化

$J(\Theta)=-{1\over m}[\sum^m_{i=1}\sum^K_{k=1}y_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\Theta(x^{(i)}))_k)]-{\lambda \over 2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_1+1}(\Theta^{(l)}_{ji})^2$

<img src="https://img-blog.csdnimg.cn/2020102601100525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVySmF5enox,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />

 `	`

## 2.2 反向传播Backpropagation

反向传播算法（Backpropagation）

$\delta_j^{(l)}$: 第$l$层神经网络的第$j$个神经元的误差

$a_j^{(l)}$: 第$l$层神经网络的第$j$个神经元的激活值

==反向传播计算流程：==

* 通过最后一层神经网络即输出层的预测值减去样本标签得出第$L$层的误差

$$ \delta^{(4)}_j=a_j^{(4)}-Labels_j=h_\Theta(x)_j-y_j=g(z^{(4)})-Labels_j$$

向量化后：$\delta^{(4)}=a^{(4)}-labels$

* $\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)} .* g^{'}(z^{(3)})$ 其中$g^{'}(z^{(3)})$ 由$a^{(3)} .* (1-a^{(3)})$推出
* $\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)} .* g^{'}(z^{(2)})$ 其中$g^{'}(z^{(2)})$ 由$a^{(2)} .* (1-a^{(2)})$推出
* 没有$\delta^{(1)}$这一项，因为第一层神经网络为输入项，不存在误差。

==一定要注意.*是矩阵的点乘，两个形状一样的矩阵对应元素相乘==

最终将推得：$${\partial\over \partial\Theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)}\delta_j^{(l+1)}$$

注意这里忽略了正则化。后续会改善

<img src="https://img-blog.csdnimg.cn/20201026011125220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVySmF5enox,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />

<img src="https://img-blog.csdnimg.cn/20201026011253204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVySmF5enox,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />

==反向传播算法思路：==

1. 定义训练数据集${(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})}$

2. 初始化误差$\Delta_{ij}^{(l)}$值

3. 遍历所有样本的循环

   3.1 定义$$a^{(1)}=x^{(i)}$$即设置输入值为第一层神经元的激活值

   3.2 前向传播计算出接下来各层的激活值$a^{(l)}$

   3.3 利用已知的样本标签，用第$L$层的预测值减去实际标签值的出$L$

层的误差值$\delta^{(L)}$

​       3.4 反向传播计算出各层的$\delta$值（除了第一层输入层没有误差）

​       3.5 计算$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$向量化：$\Delta^{(l)} := \Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$

4. 分开计算偏移项的偏导数和其他项的偏导数

<img src="https://img-blog.csdnimg.cn/20201026011144608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVySmF5enox,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />

## 2.3 梯度检验Numerical estimation of gradients

当$\theta$是实数R的情况下，双侧差分计算该点的近似导数

<img src="https://img-blog.csdnimg.cn/20201026011307465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVySmF5enox,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />

当$\theta$为向量参数的情况下，即为n维向量，计算导数近似值

<img src="https://img-blog.csdnimg.cn/20201026011319517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVySmF5enox,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:50%;" />

执行笔记：

1. 通过反向传播计算偏导数值DVec
2. 实现数值上的梯度检验，通过计算gradApprox
3. 确保DCvec和gradApprox取值近似
4. 通过检验后代码实现的时候关闭梯度检验提升计算效率

## 2.4 总结respect

训练一个神经网络

1. 选取合适的神经网络结构：隐藏层数、神经元数

   通过输出特征和输出类别数确定神经网络的第一层和最后一层的神经元个数。注意输出类别y的向量表示。

   默认使用一层隐藏层，如果使用多个隐藏层默认每个隐藏层的神经元个数一致。隐藏单元一般为输入特征数的一倍或几倍

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201026011347564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N1cGVySmF5enox,size_16,color_FFFFFF,t_70#pic_center)


==实现训练一个神经网络的步骤：==

1. 确定神经网络结构并随机初始化权重参数（很小，接近0）
2. 执行前向传播算法，对输入特征x计算出相应的输出
3. 计算损失函数$J(\Theta)$
4. 执行反向传播计算偏导数项${\partial\over \partial\Theta_ij^{(l)}}J(\Theta)$
5. 使用梯度检验比较偏导数值，确保值接近最后关闭梯度检验
6. 使用一个优化算法，例如梯度下降或更先进的算法和反向传播结合去最小话损失函数$J(\Theta)$(注意是非凸函数，可能优化至局部最优化处，但也很不错了，可以改变初始权重参数)

# 三、











